{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1684173b",
   "metadata": {},
   "source": [
    "\n",
    "# JP‑GAN (E1): I‑JEPA‑guided GAN on FFHQ‑128 (Kaggle Notebook)\n",
    "\n",
    "This notebook implements **Experiment E1 (JP‑GAN)** — a lightweight GAN trained on **FFHQ 128×128** with an **auxiliary I‑JEPA feature‑matching prior** added to the generator loss.\n",
    "\n",
    "**Idea (JP‑GAN):** alongside the standard adversarial objective, the generator minimizes a **feature moment matching loss** in a *semantic* embedding space extracted by a frozen **I‑JEPA** encoder. Concretely, in each batch we compute embeddings for real images `f(x)` and generated images `f(G(z))` and minimize the difference between **batch means and standard deviations** of these embeddings:\n",
    "\\(\n",
    "\\mathcal{L}_{\\text{JEPA}} = \\lVert \\mu_\\text{real}-\\mu_\\text{fake}\\rVert_2^2 + \\lVert \\sigma_\\text{real}-\\sigma_\\text{fake}\\rVert_2^2\n",
    "\\)\n",
    "The final generator loss is:\n",
    "\\(\n",
    "\\mathcal{L}_G = \\mathcal{L}_{\\text{adv}} + \\lambda \\cdot \\mathcal{L}_{\\text{JEPA}}.\n",
    "\\)\n",
    "\n",
    "> **Why this variant?** It’s (1) cheap and stable, (2) keeps I‑JEPA **frozen** (no extra training), and (3) avoids computing full FID-like covariances while still nudging the generator toward semantically plausible modes.\n",
    "\n",
    "## What you need to run this on Kaggle\n",
    "\n",
    "1. **Add FFHQ‑128 dataset to your notebook** (Kaggle → *Add Data* → search one of:  \n",
    "   - `dullaz/flickrfaces-dataset-nvidia-128x128`  \n",
    "   - `potatohd404/ffhq-128-70k`  \n",
    "   The loader will auto-detect images under `/kaggle/input/*/`.\n",
    "\n",
    "2. **Turn on Internet** (Notebook *Settings → Internet → On*) so Hugging Face can download the **I‑JEPA** (or fallback) weights the first time.\n",
    "\n",
    "3. Optional: lower `max_steps_per_epoch` to do a quick sanity run.\n",
    "\n",
    "> If I‑JEPA is too heavy for your GPU, the code **automatically falls back to a smaller DINO ViT‑S/16** encoder. You can force the embedder via `CFG['embedder_preference']`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694d50be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%capture\n",
    "!pip -q install --upgrade pip\n",
    "!pip -q install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu118\n",
    "!pip -q install transformers==4.43.3 timm==0.9.12 accelerate==0.33.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab1ea4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, sys, math, random, glob, time, json\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils as vutils\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import AutoImageProcessor, AutoModel, AutoModelForImageClassification\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# ---- Config ----\n",
    "CFG = {\n",
    "    \"image_size\": 128,\n",
    "    \"latent_dim\": 256,\n",
    "    \"g_channels\": 64,            # base channels for G\n",
    "    \"d_channels\": 64,            # base channels for D\n",
    "    \"batch_size\": 64,\n",
    "    \"num_workers\": 4,\n",
    "    \"epochs\": 2,                 # increase as you wish\n",
    "    \"lr_g\": 2e-4,\n",
    "    \"lr_d\": 2e-4,\n",
    "    \"betas\": (0.0, 0.9),         # good for hinge-GANs\n",
    "    \"n_crit\": 1,                 # D steps per G step\n",
    "    \"lambda_jepa\": 1.0,          # JEPA prior strength\n",
    "    \"mixed_precision\": True,\n",
    "    \"max_steps_per_epoch\": None, # e.g., 1000 for quick runs\n",
    "    \"seed\": 42,\n",
    "    # Try IJepa first; if OOM or download issues, we fallback to DINO ViT-S/16\n",
    "    \"embedder_preference\": [\"facebook/ijepa_vith14_1k\", \"facebook/ijepa_vith16_1k\", \"facebook/dino-vits16\"],\n",
    "    \"save_dir\": \"/kaggle/working/jp_gan_e1\",\n",
    "}\n",
    "\n",
    "os.makedirs(CFG[\"save_dir\"], exist_ok=True)\n",
    "\n",
    "def set_seed(s):\n",
    "    random.seed(s); torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "set_seed(CFG[\"seed\"])\n",
    "\n",
    "# Utility: simple timer\n",
    "class Timer:\n",
    "    def __enter__(self):\n",
    "        import time\n",
    "        self.t0=time.time(); return self\n",
    "    def __exit__(self,*args):\n",
    "        import time\n",
    "        self.dt=time.time()-self.t0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9ad152",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_images(root=\"/kaggle/input\"):\n",
    "    exts = {\".png\",\".jpg\",\".jpeg\",\".webp\",\".bmp\"}\n",
    "    files = []\n",
    "    for p in Path(root).glob(\"*\"):\n",
    "        if not p.is_dir(): \n",
    "            continue\n",
    "        # common subdirs in FFHQ uploads\n",
    "        candidates = [p, p/\"images\", p/\"thumbnails128x128\", p/\"ffhq-128\", p/\"ffhq\"]\n",
    "        for c in candidates:\n",
    "            if c.exists():\n",
    "                for f in c.rglob(\"*\"):\n",
    "                    if f.suffix.lower() in exts:\n",
    "                        files.append(str(f))\n",
    "    return sorted(files)\n",
    "\n",
    "class ImageFolderFlat(Dataset):\n",
    "    def __init__(self, files: List[str], size: int = 128):\n",
    "        self.files = files\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((size, size), interpolation=Image.BICUBIC),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])\n",
    "        ])\n",
    "    def __len__(self): return len(self.files)\n",
    "    def __getitem__(self, idx):\n",
    "        fp = self.files[idx]\n",
    "        im = Image.open(fp).convert(\"RGB\")\n",
    "        return self.transform(im)\n",
    "\n",
    "files = find_images()\n",
    "print(f\"Found {len(files)} image files under /kaggle/input/*\")\n",
    "assert len(files) > 0, \"No images found. Add an FFHQ-128 Kaggle dataset to the notebook (see intro cell).\"\n",
    "\n",
    "ds = ImageFolderFlat(files, size=CFG[\"image_size\"])\n",
    "dl = DataLoader(ds, batch_size=CFG[\"batch_size\"], shuffle=True, num_workers=CFG[\"num_workers\"], pin_memory=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07a74c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ResBlockUp(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, 1, 1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, 1, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_ch)\n",
    "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
    "        self.skip = nn.Conv2d(in_ch, out_ch, 1)\n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "        h = self.bn1(self.conv1(x)); h = F.relu(h, inplace=True)\n",
    "        h = self.bn2(self.conv2(h)); h = F.relu(h, inplace=True)\n",
    "        s = self.skip(x)\n",
    "        return h + s\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim=256, base=64, img_size=128):\n",
    "        super().__init__()\n",
    "        self.init_sz = img_size // 16  # 8 for 128x128\n",
    "        self.fc = nn.Linear(z_dim, base*8*self.init_sz*self.init_sz)\n",
    "        ch = base*8\n",
    "        blocks = []\n",
    "        for _ in range(4): # x2 four times: 8->16->32->64->128\n",
    "            blocks.append(ResBlockUp(ch, ch//2))\n",
    "            ch //= 2\n",
    "        self.blocks = nn.Sequential(*blocks)\n",
    "        self.to_rgb = nn.Conv2d(ch, 3, 3, 1, 1)\n",
    "    def forward(self, z):\n",
    "        h = self.fc(z).view(z.size(0), -1, self.init_sz, self.init_sz)\n",
    "        h = self.blocks(h)\n",
    "        x = torch.tanh(self.to_rgb(h))\n",
    "        return x\n",
    "\n",
    "def SN(module): \n",
    "    return nn.utils.spectral_norm(module)\n",
    "\n",
    "class DiscBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, down=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = SN(nn.Conv2d(in_ch, out_ch, 3, 1, 1))\n",
    "        self.conv2 = SN(nn.Conv2d(out_ch, out_ch, 3, 1, 1))\n",
    "        self.down = down\n",
    "        self.skip = SN(nn.Conv2d(in_ch, out_ch, 1, 1, 0))\n",
    "    def forward(self, x):\n",
    "        h = F.leaky_relu(self.conv1(x), 0.2, inplace=True)\n",
    "        h = F.leaky_relu(self.conv2(h), 0.2, inplace=True)\n",
    "        if self.down:\n",
    "            h = F.avg_pool2d(h, 2)\n",
    "            s = F.avg_pool2d(self.skip(x), 2)\n",
    "        else:\n",
    "            s = self.skip(x)\n",
    "        return h + s\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, base=64, img_size=128):\n",
    "        super().__init__()\n",
    "        ch = base\n",
    "        blocks = [DiscBlock(3, ch, down=True)]\n",
    "        for _ in range(3): # 128->64->32->16->8\n",
    "            blocks.append(DiscBlock(ch, ch*2, down=True))\n",
    "            ch *= 2\n",
    "        blocks.append(DiscBlock(ch, ch, down=False))\n",
    "        self.blocks = nn.Sequential(*blocks)\n",
    "        self.linear = SN(nn.Linear(ch*8*8, 1))\n",
    "    def forward(self, x):\n",
    "        h = self.blocks(x)\n",
    "        h = h.view(h.size(0), -1)\n",
    "        out = self.linear(h)\n",
    "        return out.squeeze(1)\n",
    "\n",
    "G = Generator(z_dim=CFG[\"latent_dim\"], base=CFG[\"g_channels\"], img_size=CFG[\"image_size\"]).to(DEVICE)\n",
    "D = Discriminator(base=CFG[\"d_channels\"], img_size=CFG[\"image_size\"]).to(DEVICE)\n",
    "print(f\"G params: {sum(p.numel() for p in G.parameters())/1e6:.2f}M | D params: {sum(p.numel() for p in D.parameters())/1e6:.2f}M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0a3a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_embedder(preference_list):\n",
    "    last_err = None\n",
    "    for name in preference_list:\n",
    "        try:\n",
    "            print(f\"Trying embedder: {name}\")\n",
    "            processor = AutoImageProcessor.from_pretrained(name, trust_remote_code=True)\n",
    "            model = AutoModel.from_pretrained(name, trust_remote_code=True)\n",
    "            model.eval().to(DEVICE)\n",
    "            # Small probe to determine pooling: we use mean of last_hidden_state if present, else penultimate features\n",
    "            def featurize(x):  # x in [-1,1], Bx3xHxW\n",
    "                with torch.no_grad():\n",
    "                    # Map to [0,1] and resize to expected size\n",
    "                    imgs = (x * 0.5 + 0.5).clamp(0,1)\n",
    "                    # transformers expects list of PIL or numpy; but pixel_values works too\n",
    "                    inputs = processor(images=[transforms.ToPILImage()(img) for img in imgs.cpu()], return_tensors=\"pt\").to(DEVICE)\n",
    "                    out = model(**inputs)\n",
    "                if hasattr(out, \"last_hidden_state\"):\n",
    "                    feats = out.last_hidden_state.mean(dim=1)  # mean pool tokens\n",
    "                elif hasattr(out, \"pooler_output\") and out.pooler_output is not None:\n",
    "                    feats = out.pooler_output\n",
    "                else:\n",
    "                    # Fallback: try extracting from logits if it's a classifier model\n",
    "                    logits = getattr(out, \"logits\", None)\n",
    "                    if logits is None:\n",
    "                        raise RuntimeError(\"Unsupported model outputs for feature extraction.\")\n",
    "                    feats = logits\n",
    "                return feats\n",
    "            # smoke test on a tiny random batch\n",
    "            _ = featurize(torch.randn(2,3,CFG[\"image_size\"],CFG[\"image_size\"], device=DEVICE))\n",
    "            print(f\"Loaded embedder: {name}\")\n",
    "            return processor, model, featurize, name\n",
    "        except Exception as e:\n",
    "            print(\"Failed to load\", name, \"because:\", repr(e))\n",
    "            last_err = e\n",
    "    raise last_err\n",
    "\n",
    "processor, embedder, featurize_nograd, embedder_name = load_embedder(CFG[\"embedder_preference\"])\n",
    "\n",
    "# Version with gradient for fake images (so G receives signal). We detach only the real features.\n",
    "def featurize_with_grad(x):\n",
    "    # No torch.no_grad here. We still wrap pre/post steps as needed.\n",
    "    imgs = (x * 0.5 + 0.5).clamp(0,1)\n",
    "    inputs = processor(images=[transforms.ToPILImage()(img) for img in imgs.detach().cpu()], return_tensors=\"pt\").to(DEVICE)\n",
    "    out = embedder(**inputs)\n",
    "    if hasattr(out, \"last_hidden_state\"):\n",
    "        feats = out.last_hidden_state.mean(dim=1)\n",
    "    elif hasattr(out, \"pooler_output\") and out.pooler_output is not None:\n",
    "        feats = out.pooler_output\n",
    "    else:\n",
    "        logits = getattr(out, \"logits\", None)\n",
    "        if logits is None:\n",
    "            raise RuntimeError(\"Unsupported model outputs for feature extraction.\")\n",
    "        feats = logits\n",
    "    return feats\n",
    "\n",
    "print(\"Embedder selected:\", embedder_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23c1afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "opt_g = torch.optim.Adam(G.parameters(), lr=CFG[\"lr_g\"], betas=CFG[\"betas\"])\n",
    "opt_d = torch.optim.Adam(D.parameters(), lr=CFG[\"lr_d\"], betas=CFG[\"betas\"])\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=CFG[\"mixed_precision\"])\n",
    "\n",
    "def d_hinge_loss(real_logits, fake_logits):\n",
    "    loss_real = F.relu(1.0 - real_logits).mean()\n",
    "    loss_fake = F.relu(1.0 + fake_logits).mean()\n",
    "    return loss_real + loss_fake\n",
    "\n",
    "def g_hinge_loss(fake_logits):\n",
    "    return -fake_logits.mean()\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_grid(n=64):\n",
    "    G.eval()\n",
    "    z = torch.randn(n, CFG[\"latent_dim\"], device=DEVICE)\n",
    "    x = G(z).clamp(-1,1)\n",
    "    G.train()\n",
    "    grid = vutils.make_grid((x*0.5+0.5), nrow=int(math.sqrt(n)))\n",
    "    out_path = os.path.join(CFG[\"save_dir\"], f\"samples_{int(time.time())}.png\")\n",
    "    vutils.save_image(grid, out_path)\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079d7f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def jepa_feature_moment_loss(x_real, x_fake):\n",
    "    # x_* in [-1,1], Bx3xHxW\n",
    "    with torch.no_grad():\n",
    "        f_real = featurize_nograd(x_real)  # detach for real; no grad needed\n",
    "    f_fake = featurize_with_grad(x_fake)   # allow grad to flow into G via embedder\n",
    "    mu_r, mu_f = f_real.mean(0), f_fake.mean(0)\n",
    "    sd_r, sd_f = f_real.std(0),  f_fake.std(0)\n",
    "    return (mu_r - mu_f).pow(2).mean() + (sd_r - sd_f).pow(2).mean()\n",
    "\n",
    "def train():\n",
    "    global G, D\n",
    "    step = 0\n",
    "    best_seen = None\n",
    "    for epoch in range(1, CFG[\"epochs\"]+1):\n",
    "        running = {\"d\":0.0, \"g\":0.0, \"jepa\":0.0}\n",
    "        with Timer() as t_epoch:\n",
    "            for i, real in enumerate(dl):\n",
    "                if CFG[\"max_steps_per_epoch\"] is not None and i >= CFG[\"max_steps_per_epoch\"]:\n",
    "                    break\n",
    "                real = real.to(DEVICE, non_blocking=True)\n",
    "                bs = real.size(0)\n",
    "                z = torch.randn(bs, CFG[\"latent_dim\"], device=DEVICE)\n",
    "                # ------------------ Train D ------------------\n",
    "                for _ in range(CFG[\"n_crit\"]):\n",
    "                    with torch.cuda.amp.autocast(enabled=CFG[\"mixed_precision\"]):\n",
    "                        fake = G(z).detach()\n",
    "                        d_real = D(real)\n",
    "                        d_fake = D(fake)\n",
    "                        loss_d = d_hinge_loss(d_real, d_fake)\n",
    "                    opt_d.zero_grad(set_to_none=True)\n",
    "                    scaler.scale(loss_d).backward()\n",
    "                    scaler.step(opt_d)\n",
    "                    # no scaler.update() here, we do once per iteration after G update\n",
    "\n",
    "                # ------------------ Train G ------------------\n",
    "                z = torch.randn(bs, CFG[\"latent_dim\"], device=DEVICE)\n",
    "                with torch.cuda.amp.autocast(enabled=CFG[\"mixed_precision\"]):\n",
    "                    fake = G(z)\n",
    "                    g_adv = g_hinge_loss(D(fake))\n",
    "                    jepa = jepa_feature_moment_loss(real, fake) * CFG[\"lambda_jepa\"]\n",
    "                    loss_g = g_adv + jepa\n",
    "                opt_g.zero_grad(set_to_none=True)\n",
    "                scaler.scale(loss_g).backward()\n",
    "                scaler.step(opt_g)\n",
    "                scaler.update()\n",
    "\n",
    "                # Logs\n",
    "                running[\"d\"] += float(loss_d.detach().cpu())\n",
    "                running[\"g\"] += float(g_adv.detach().cpu())\n",
    "                running[\"jepa\"] += float((jepa.detach().cpu()))\n",
    "\n",
    "                if (step % 200) == 0:\n",
    "                    path = sample_grid(36)\n",
    "                    print(f\"[ep {epoch:02d} | it {i:05d}] D={running['d']/(i+1):.3f}  G={running['g']/(i+1):.3f}  JEPA={running['jepa']/(i+1):.3f}  → saved {path}\")\n",
    "                step += 1\n",
    "\n",
    "        # Save checkpoint per epoch\n",
    "        ckpt = {\n",
    "            \"G\": G.state_dict(),\n",
    "            \"D\": D.state_dict(),\n",
    "            \"opt_g\": opt_g.state_dict(),\n",
    "            \"opt_d\": opt_d.state_dict(),\n",
    "            \"cfg\": CFG,\n",
    "            \"embedder\": embedder_name,\n",
    "        }\n",
    "        torch.save(ckpt, os.path.join(CFG[\"save_dir\"], f\"ckpt_e{epoch:02d}.pt\"))\n",
    "        print(f\"Epoch {epoch} done in {t_epoch.dt:.1f}s. Checkpoint saved.\")\n",
    "\n",
    "train()\n",
    "print(\"Training finished.\")\n",
    "print(\"Sample preview:\", sample_grid(64))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}