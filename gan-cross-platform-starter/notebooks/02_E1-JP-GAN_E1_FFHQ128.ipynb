{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f66eaa73",
   "metadata": {},
   "source": [
    "\n",
    "# JPâ€‘GAN (E1) â€” FFHQâ€‘128 â€” **Iâ€‘JEPA enabled (HF)** + metrics + 2Ã—GPU + baseline vs JPâ€‘GAN\n",
    "\n",
    "**Now with native Iâ€‘JEPA support via ðŸ¤—Transformers (>= 4.49).**  \n",
    "This notebook runs two configurations â€” **baseline** (Î»=0) and **JPâ€‘GAN** (Î»>0) â€” and reports **FID/KID/IS**. It will:\n",
    "- try to load **`facebook/ijepa_vith14_1k`** first (feature extractor),\n",
    "- if unavailable, automatically **fallback to DINOv2** (or smaller ViT).\n",
    "\n",
    "> If two T4 GPUs are available, **G** and **D** run on both via `nn.DataParallel`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd7e94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%capture\n",
    "!pip -q install --upgrade pip\n",
    "!pip -q install \"transformers>=4.53.3\" timm==0.9.12 accelerate==0.33.0\n",
    "!pip -q install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu118\n",
    "!pip -q install torchmetrics==1.4.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9191b4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, sys, math, random, glob, time, json\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, List, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils as vutils\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchmetrics.image.kid import KernelInceptionDistance\n",
    "from torchmetrics.image.inception import InceptionScore\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "N_GPUS = torch.cuda.device_count()\n",
    "print(\"Device:\", DEVICE, \"| #GPUs:\", N_GPUS)\n",
    "\n",
    "CFG = {\n",
    "    \"image_size\": 128,\n",
    "    \"latent_dim\": 256,\n",
    "    \"g_channels\": 64,\n",
    "    \"d_channels\": 64,\n",
    "    \"batch_size\": 64,\n",
    "    \"num_workers\": 4,\n",
    "    \"epochs\": 2,\n",
    "    \"lr_g\": 2e-4,\n",
    "    \"lr_d\": 2e-4,\n",
    "    \"betas\": (0.0, 0.9),\n",
    "    \"n_crit\": 1,\n",
    "    \"lambda_jepa\": 1.0,\n",
    "    \"mixed_precision\": True,\n",
    "    \"max_steps_per_epoch\": None,\n",
    "    \"seed\": 42,\n",
    "    \"runs\": [\n",
    "        {\"name\": \"baseline\", \"lambda_jepa\": 0.0},\n",
    "        {\"name\": \"jpgan\",    \"lambda_jepa\": 1.0},\n",
    "    ],\n",
    "    \"embedder_preference\": [\"facebook/ijepa_vith14_1k\", \"facebook/dinov2-base\", \"facebook/dino-vits16\", \"google/vit-base-patch16-224\"],\n",
    "    \"save_dir\": \"/kaggle/working/jp_gan_e1_ijepa\",\n",
    "    \"eval_num_real\": 2000,\n",
    "    \"eval_num_fake\": 2000,\n",
    "}\n",
    "\n",
    "os.makedirs(CFG[\"save_dir\"], exist_ok=True)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def set_seed(s):\n",
    "    random.seed(s); torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
    "set_seed(CFG[\"seed\"])\n",
    "\n",
    "class Timer:\n",
    "    def __enter__(self): self.t0=time.time(); return self\n",
    "    def __exit__(self,*a): self.dt=time.time()-self.t0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b053998c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_images(root=\"/kaggle/input\"):\n",
    "    exts = {\".png\",\".jpg\",\".jpeg\",\".webp\",\".bmp\"}\n",
    "    files = []\n",
    "    for p in Path(root).glob(\"*\"):\n",
    "        if not p.is_dir(): \n",
    "            continue\n",
    "        candidates = [p, p/\"images\", p/\"thumbnails128x128\", p/\"ffhq-128\", p/\"ffhq\"]\n",
    "        for c in candidates:\n",
    "            if c.exists():\n",
    "                for f in c.rglob(\"*\"):\n",
    "                    if f.suffix.lower() in exts:\n",
    "                        files.append(str(f))\n",
    "    return sorted(files)\n",
    "\n",
    "class ImageFolderFlat(Dataset):\n",
    "    def __init__(self, files: List[str], size: int = 128):\n",
    "        self.files = files\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((size, size), interpolation=Image.BICUBIC),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])\n",
    "        ])\n",
    "    def __len__(self): return len(self.files)\n",
    "    def __getitem__(self, idx):\n",
    "        fp = self.files[idx]\n",
    "        im = Image.open(fp).convert(\"RGB\")\n",
    "        return self.transform(im)\n",
    "\n",
    "files = find_images()\n",
    "print(f\"Found {len(files)} image files under /kaggle/input/*\")\n",
    "assert len(files) > 0, \"No images found. Add an FFHQ-128 dataset via 'Add data'.\"\n",
    "\n",
    "ds = ImageFolderFlat(files, size=CFG[\"image_size\"])\n",
    "dl = DataLoader(ds, batch_size=CFG[\"batch_size\"], shuffle=True, num_workers=CFG[\"num_workers\"], pin_memory=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c0ed76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ResBlockUp(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, 1, 1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, 1, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_ch)\n",
    "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
    "        self.skip = nn.Conv2d(in_ch, out_ch, 1)\n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "        h = self.bn1(self.conv1(x)); h = F.relu(h, inplace=True)\n",
    "        h = self.bn2(self.conv2(h)); h = F.relu(h, inplace=True)\n",
    "        s = self.skip(x)\n",
    "        return h + s\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim=256, base=64, img_size=128):\n",
    "        super().__init__()\n",
    "        self.init_sz = img_size // 16\n",
    "        self.fc = nn.Linear(z_dim, base*8*self.init_sz*self.init_sz)\n",
    "        ch = base*8\n",
    "        blocks = []\n",
    "        for _ in range(4):\n",
    "            blocks.append(ResBlockUp(ch, ch//2))\n",
    "            ch //= 2\n",
    "        self.blocks = nn.Sequential(*blocks)\n",
    "        self.to_rgb = nn.Conv2d(ch, 3, 3, 1, 1)\n",
    "    def forward(self, z):\n",
    "        h = self.fc(z).view(z.size(0), -1, self.init_sz, self.init_sz)\n",
    "        h = self.blocks(h)\n",
    "        x = torch.tanh(self.to_rgb(h))\n",
    "        return x\n",
    "\n",
    "def SN(m): return nn.utils.spectral_norm(m)\n",
    "\n",
    "class DiscBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, down=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = SN(nn.Conv2d(in_ch, out_ch, 3, 1, 1))\n",
    "        self.conv2 = SN(nn.Conv2d(out_ch, out_ch, 3, 1, 1))\n",
    "        self.down = down\n",
    "        self.skip = SN(nn.Conv2d(in_ch, out_ch, 1, 1, 0))\n",
    "    def forward(self, x):\n",
    "        h = F.leaky_relu(self.conv1(x), 0.2, inplace=True)\n",
    "        h = F.leaky_relu(self.conv2(h), 0.2, inplace=True)\n",
    "        if self.down:\n",
    "            h = F.avg_pool2d(h, 2)\n",
    "            s = F.avg_pool2d(self.skip(x), 2)\n",
    "        else:\n",
    "            s = self.skip(x)\n",
    "        return h + s\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, base=64, img_size=128):\n",
    "        super().__init__()\n",
    "        ch = base\n",
    "        blocks = [DiscBlock(3, ch, down=True)]\n",
    "        for _ in range(3):\n",
    "            blocks.append(DiscBlock(ch, ch*2, down=True))\n",
    "            ch *= 2\n",
    "        blocks.append(DiscBlock(ch, ch, down=False))\n",
    "        self.blocks = nn.Sequential(*blocks)\n",
    "        self.linear = SN(nn.Linear(ch*8*8, 1))\n",
    "    def forward(self, x):\n",
    "        h = self.blocks(x)\n",
    "        h = h.view(h.size(0), -1)\n",
    "        out = self.linear(h)\n",
    "        return out.squeeze(1)\n",
    "\n",
    "G = Generator(z_dim=CFG[\"latent_dim\"], base=CFG[\"g_channels\"], img_size=CFG[\"image_size\"])\n",
    "D = Discriminator(base=CFG[\"d_channels\"], img_size=CFG[\"image_size\"])\n",
    "\n",
    "if DEVICE == \"cuda\" and torch.cuda.device_count() > 1:\n",
    "    print(\"Using DataParallel on\", torch.cuda.device_count(), \"GPUs for G and D\")\n",
    "    G = nn.DataParallel(G).to(DEVICE)\n",
    "    D = nn.DataParallel(D).to(DEVICE)\n",
    "else:\n",
    "    G = G.to(DEVICE); D = D.to(DEVICE)\n",
    "\n",
    "def unwrap(m):\n",
    "    return m.module if isinstance(m, nn.DataParallel) else m\n",
    "\n",
    "print(f\"G params: {sum(p.numel() for p in unwrap(G).parameters())/1e6:.2f}M | D params: {sum(p.numel() for p in unwrap(D).parameters())/1e6:.2f}M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0697da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_embedder(preference_list):\n",
    "    last_err = None\n",
    "    for name in preference_list:\n",
    "        try:\n",
    "            print(f\"Trying embedder: {name}\")\n",
    "            processor = AutoProcessor.from_pretrained(name, trust_remote_code=True)\n",
    "            model = AutoModel.from_pretrained(name, trust_remote_code=True)\n",
    "            model.eval().to(DEVICE)\n",
    "            def featurize_nograd(x):\n",
    "                with torch.no_grad():\n",
    "                    imgs = (x * 0.5 + 0.5).clamp(0,1)\n",
    "                    pil_list = [transforms.ToPILImage()(img) for img in imgs.cpu()]\n",
    "                    inputs = processor(images=pil_list, return_tensors=\"pt\").to(DEVICE)\n",
    "                    out = model(**inputs)\n",
    "                if hasattr(out, \"last_hidden_state\"):\n",
    "                    feats = out.last_hidden_state.mean(dim=1)\n",
    "                elif hasattr(out, \"pooler_output\") and out.pooler_output is not None:\n",
    "                    feats = out.pooler_output\n",
    "                else:\n",
    "                    logits = getattr(out, \"logits\", None)\n",
    "                    if logits is None:\n",
    "                        raise RuntimeError(\"Unsupported model outputs for feature extraction.\")\n",
    "                    feats = logits\n",
    "                return feats\n",
    "            def featurize_with_grad(x):\n",
    "                imgs = (x * 0.5 + 0.5).clamp(0,1)\n",
    "                pil_list = [transforms.ToPILImage()(img) for img in imgs.detach().cpu()]\n",
    "                inputs = processor(images=pil_list, return_tensors=\"pt\").to(DEVICE)\n",
    "                out = model(**inputs)\n",
    "                if hasattr(out, \"last_hidden_state\"):\n",
    "                    feats = out.last_hidden_state.mean(dim=1)\n",
    "                elif hasattr(out, \"pooler_output\") and out.pooler_output is not None:\n",
    "                    feats = out.pooler_output\n",
    "                else:\n",
    "                    logits = getattr(out, \"logits\", None)\n",
    "                    if logits is None:\n",
    "                        raise RuntimeError(\"Unsupported model outputs for feature extraction.\")\n",
    "                    feats = logits\n",
    "                return feats\n",
    "            _ = featurize_nograd(torch.randn(2,3,CFG[\"image_size\"],CFG[\"image_size\"], device=DEVICE))\n",
    "            print(f\"Loaded embedder: {name}\")\n",
    "            return processor, model, featurize_nograd, featurize_with_grad, name\n",
    "        except Exception as e:\n",
    "            print(\"Failed to load\", name, \"because:\", repr(e))\n",
    "            last_err = e\n",
    "    raise last_err\n",
    "\n",
    "processor, embedder_model, featurize_nograd, featurize_with_grad, embedder_name = load_embedder(CFG[\"embedder_preference\"])\n",
    "print(\"Embedder selected:\", embedder_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25fe6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "opt_g = torch.optim.Adam(unwrap(G).parameters(), lr=CFG[\"lr_g\"], betas=CFG[\"betas\"])\n",
    "opt_d = torch.optim.Adam(unwrap(D).parameters(), lr=CFG[\"lr_d\"], betas=CFG[\"betas\"])\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=CFG[\"mixed_precision\"])\n",
    "\n",
    "def d_hinge_loss(real_logits, fake_logits):\n",
    "    return F.relu(1.0 - real_logits).mean() + F.relu(1.0 + fake_logits).mean()\n",
    "\n",
    "def g_hinge_loss(fake_logits):\n",
    "    return -fake_logits.mean()\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_grid(G_model, n=64, fname_prefix=\"samples\"):\n",
    "    was_train = G_model.training\n",
    "    G_model.eval()\n",
    "    z = torch.randn(n, CFG[\"latent_dim\"], device=DEVICE)\n",
    "    x = G_model(z).clamp(-1,1)\n",
    "    if was_train: G_model.train()\n",
    "    grid = vutils.make_grid((x*0.5+0.5), nrow=int(math.sqrt(n)))\n",
    "    out_path = os.path.join(CFG[\"save_dir\"], f\"{fname_prefix}_{int(time.time())}.png\")\n",
    "    vutils.save_image(grid, out_path)\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d11dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def jepa_feature_moment_loss(x_real, x_fake):\n",
    "    with torch.no_grad():\n",
    "        f_real = featurize_nograd(x_real)\n",
    "    f_fake = featurize_with_grad(x_fake)\n",
    "    mu_r, mu_f = f_real.mean(0), f_fake.mean(0)\n",
    "    sd_r, sd_f = f_real.std(0),  f_fake.std(0)\n",
    "    return (mu_r - mu_f).pow(2).mean() + (sd_r - sd_f).pow(2).mean()\n",
    "\n",
    "def run_training(run_cfg):\n",
    "    lambda_jepa = run_cfg[\"lambda_jepa\"]\n",
    "    run_name = run_cfg[\"name\"]\n",
    "    print(f\"\\n=== Run: {run_name} | lambda_jepa={lambda_jepa} | embedder={embedder_name} ===\")\n",
    "    global G, D, opt_g, opt_d\n",
    "    G = Generator(z_dim=CFG[\"latent_dim\"], base=CFG[\"g_channels\"], img_size=CFG[\"image_size\"])\n",
    "    D = Discriminator(base=CFG[\"d_channels\"], img_size=CFG[\"image_size\"])\n",
    "    if DEVICE == \"cuda\" and torch.cuda.device_count() > 1:\n",
    "        G = nn.DataParallel(G).to(DEVICE)\n",
    "        D = nn.DataParallel(D).to(DEVICE)\n",
    "    else:\n",
    "        G = G.to(DEVICE); D = D.to(DEVICE)\n",
    "    opt_g = torch.optim.Adam(unwrap(G).parameters(), lr=CFG[\"lr_g\"], betas=CFG[\"betas\"])\n",
    "    opt_d = torch.optim.Adam(unwrap(D).parameters(), lr=CFG[\"lr_d\"], betas=CFG[\"betas\"])\n",
    "\n",
    "    step = 0\n",
    "    for epoch in range(1, CFG[\"epochs\"]+1):\n",
    "        running = {\"d\":0.0, \"g\":0.0, \"jepa\":0.0}\n",
    "        with Timer() as t_epoch:\n",
    "            for i, real in enumerate(dl):\n",
    "                if CFG[\"max_steps_per_epoch\"] is not None and i >= CFG[\"max_steps_per_epoch\"]:\n",
    "                    break\n",
    "                real = real.to(DEVICE, non_blocking=True)\n",
    "                bs = real.size(0)\n",
    "                z = torch.randn(bs, CFG[\"latent_dim\"], device=DEVICE)\n",
    "                for _ in range(CFG[\"n_crit\"]):\n",
    "                    with torch.cuda.amp.autocast(enabled=CFG[\"mixed_precision\"]):\n",
    "                        fake = G(z).detach()\n",
    "                        d_real = D(real)\n",
    "                        d_fake = D(fake)\n",
    "                        loss_d = d_hinge_loss(d_real, d_fake)\n",
    "                    opt_d.zero_grad(set_to_none=True)\n",
    "                    scaler.scale(loss_d).backward()\n",
    "                    scaler.step(opt_d)\n",
    "                z = torch.randn(bs, CFG[\"latent_dim\"], device=DEVICE)\n",
    "                with torch.cuda.amp.autocast(enabled=CFG[\"mixed_precision\"]):\n",
    "                    fake = G(z)\n",
    "                    g_adv = g_hinge_loss(D(fake))\n",
    "                    jepa = jepa_feature_moment_loss(real, fake) * lambda_jepa\n",
    "                    loss_g = g_adv + jepa\n",
    "                opt_g.zero_grad(set_to_none=True)\n",
    "                scaler.scale(loss_g).backward()\n",
    "                scaler.step(opt_g)\n",
    "                scaler.update()\n",
    "\n",
    "                running[\"d\"] += float(loss_d.detach().cpu())\n",
    "                running[\"g\"] += float(g_adv.detach().cpu())\n",
    "                running[\"jepa\"] += float((jepa.detach().cpu()))\n",
    "\n",
    "                if (step % 200) == 0:\n",
    "                    path = sample_grid(G, 36, fname_prefix=f\"{run_name}_samples\")\n",
    "                    print(f\"[{run_name}] ep {epoch:02d} it {i:05d} | D={running['d']/(i+1):.3f}  G={running['g']/(i+1):.3f}  JEPA={running['jepa']/(i+1):.3f}  â†’ {path}\")\n",
    "                step += 1\n",
    "\n",
    "        ckpt = {\n",
    "            \"G\": unwrap(G).state_dict(),\n",
    "            \"D\": unwrap(D).state_dict(),\n",
    "            \"opt_g\": opt_g.state_dict(),\n",
    "            \"opt_d\": opt_d.state_dict(),\n",
    "            \"cfg\": CFG,\n",
    "            \"run_cfg\": run_cfg,\n",
    "            \"embedder\": embedder_name,\n",
    "        }\n",
    "        out_ckpt = os.path.join(CFG[\"save_dir\"], f\"{run_name}_ckpt_e{epoch:02d}.pt\")\n",
    "        torch.save(ckpt, out_ckpt)\n",
    "        print(f\"[{run_name}] Epoch {epoch} done in {t_epoch.dt:.1f}s. Saved: {out_ckpt}\")\n",
    "\n",
    "    final_img = sample_grid(G, 64, fname_prefix=f\"{run_name}_final\")\n",
    "    print(f\"[{run_name}] Final sample grid: {final_img}\")\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cb0c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_metrics(G_model, num_real=1000, num_fake=1000, batch_size=64):\n",
    "    print(f\"Evaluating metrics on {num_real} real / {num_fake} fake images...\")\n",
    "    fid = FrechetInceptionDistance(feature=2048, normalize=True).to(DEVICE)\n",
    "    kid = KernelInceptionDistance(subset_size=1000, normalize=True).to(DEVICE)\n",
    "    isc = InceptionScore(splits=10).to(DEVICE)\n",
    "\n",
    "    n_real = 0\n",
    "    for real in dl:\n",
    "        real = real.to(DEVICE, non_blocking=True)\n",
    "        imgs = (real*0.5+0.5).clamp(0,1)\n",
    "        fid.update(imgs, real=True)\n",
    "        kid.update(imgs, real=True)\n",
    "        n_real += real.size(0)\n",
    "        if n_real >= num_real: break\n",
    "\n",
    "    n_fake = 0\n",
    "    while n_fake < num_fake:\n",
    "        bs = min(batch_size, num_fake - n_fake)\n",
    "        z = torch.randn(bs, CFG[\"latent_dim\"], device=DEVICE)\n",
    "        fake = G_model(z)\n",
    "        imgs = (fake*0.5+0.5).clamp(0,1)\n",
    "        fid.update(imgs, real=False)\n",
    "        kid.update(imgs, real=False)\n",
    "        isc.update(imgs)\n",
    "        n_fake += bs\n",
    "\n",
    "    fid_val = float(fid.compute().cpu())\n",
    "    kid_mean, kid_std = kid.compute()\n",
    "    is_mean, is_std   = isc.compute()\n",
    "    res = {\n",
    "        \"FID\": fid_val,\n",
    "        \"KID_mean\": float(kid_mean.cpu()),\n",
    "        \"KID_std\": float(kid_std.cpu()),\n",
    "        \"IS_mean\": float(is_mean.cpu()),\n",
    "        \"IS_std\": float(is_std.cpu()),\n",
    "        \"num_real\": n_real,\n",
    "        \"num_fake\": n_fake,\n",
    "        \"embedder\": embedder_name,\n",
    "    }\n",
    "    print(\"Metrics:\", res)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc004162",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_results = {}\n",
    "for run in CFG[\"runs\"]:\n",
    "    G_trained = run_training(run)\n",
    "    res = evaluate_metrics(G_trained, num_real=CFG[\"eval_num_real\"], num_fake=CFG[\"eval_num_fake\"], batch_size=CFG[\"batch_size\"])\n",
    "    all_results[run[\"name\"]] = res\n",
    "\n",
    "summary_path = os.path.join(CFG[\"save_dir\"], \"metrics_summary.json\")\n",
    "with open(summary_path, \"w\") as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "print(\"Saved metrics summary to:\", summary_path)\n",
    "print(json.dumps(all_results, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dc2cbb",
   "metadata": {},
   "source": [
    "\n",
    "### References\n",
    "- **Iâ€‘JEPA in ðŸ¤—Transformers** (v4.49+ docs, example with `facebook/ijepa_vith14_1k`).  \n",
    "- **Iâ€‘JEPA model card**: `facebook/ijepa_vith14_1k` (feature extractor).  \n",
    "- **Original paper (CVPR 2023)**.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}