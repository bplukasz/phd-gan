{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code]\n# BOOTSTRAP: odtwórz repo GAN Kaggle ⇄ Colab ⇄ RunPod/PLGrid w bieżącym środowisku\nimport os, json, textwrap, pathlib\n\nBASE = pathlib.Path(\"gan-cross-platform-starter\")\n(BASE / \"utils\").mkdir(parents=True, exist_ok=True)\n(BASE / \"notebooks\").mkdir(parents=True, exist_ok=True)\n\n# .gitignore\nopen(BASE/\".gitignore\",\"w\").write(textwrap.dedent(\"\"\"\n__pycache__/\n*.py[cod]\n*$py.class\n.venv/\nvenv/\nENV/\n.ipynb_checkpoints\ndata/\ncheckpoints/\nlogs/\nwandb/\nkaggle.json\n.DS_Store\nbuild/\ndist/\n*.egg-info/\n\"\"\").strip()+\"\\n\")\n\n# requirements.txt (bez pinowania torch/torchvision)\nopen(BASE/\"requirements.txt\",\"w\").write(textwrap.dedent(\"\"\"\nkaggle>=1.6.17\nhuggingface_hub>=0.23.0\nwandb>=0.17.0\ntqdm>=4.66.0\nomegaconf>=2.3.0\n\"\"\").strip()+\"\\n\")\n\n# README.md\nopen(BASE/\"README.md\",\"w\").write(textwrap.dedent(\"\"\"\n# GAN Cross-Platform Starter (Kaggle ⇄ Colab ⇄ RunPod/PLGrid)\n\nStart na Kaggle → kontynuacja na Colab → wznowienie gdziekolwiek (RunPod/PLGrid).\nW pakiecie: mini-DCGAN + checkpointy + sync (Kaggle Datasets / HF Hub / W&B).\n\n**Odpal:** notebooks/01_kaggle_colab_runpod.ipynb\n\"\"\").strip()+\"\\n\")\n\n# utils/checkpoint.py\nopen(BASE/\"utils/checkpoint.py\",\"w\").write(textwrap.dedent(\"\"\"\nimport os, glob, torch\n\ndef latest_checkpoint(path=\"checkpoints\", pattern=\"ckpt_*.pt\"):\n    os.makedirs(path, exist_ok=True)\n    files = sorted(glob.glob(os.path.join(path, pattern)))\n    return files[-1] if files else None\n\ndef save_checkpoint(path, state):\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n    torch.save(state, path)\n\ndef load_checkpoint(path, map_location=\"cpu\"):\n    return torch.load(path, map_location=map_location)\n\"\"\").strip()+\"\\n\")\n\n# utils/sync_kaggle.py\nopen(BASE/\"utils/sync_kaggle.py\",\"w\").write(textwrap.dedent(r'''\nimport os, subprocess, shutil\nfrom pathlib import Path\n\ndef _ensure_kaggle_config():\n    cfg1 = os.path.expanduser(\"~/.kaggle/kaggle.json\")\n    if not os.path.exists(cfg1) and not (os.getenv(\"KAGGLE_USERNAME\") and os.getenv(\"KAGGLE_KEY\")):\n        raise RuntimeError(\"Missing Kaggle credentials. Provide ~/.kaggle/kaggle.json or env KAGGLE_USERNAME/KAGGLE_KEY.\")\n\ndef kaggle_dataset_push(dataset_slug, folder=\"checkpoints\", title=None, is_public=False):\n    \"\"\"\n    Create or version a Kaggle Dataset from a local folder.\n    dataset_slug: 'user/your-dataset-slug' OR 'your-dataset-slug' if user is implied by kaggle config.\n    \"\"\"\n    _ensure_kaggle_config()\n    folder = Path(folder); folder.mkdir(parents=True, exist_ok=True)\n    meta_dir = Path(\"kaggle_meta\"); meta_dir.mkdir(exist_ok=True)\n    ds_slug_only = dataset_slug.split(\"/\")[-1]\n    title = title or ds_slug_only.replace(\"-\", \" \").title()\n    (meta_dir / \"dataset-metadata.json\").write_text(\n        f'{{\"title\":\"{title}\",\"id\":\"{dataset_slug}\",\"licenses\":[{{\"name\":\"CC0-1.0\"}}]}}', encoding=\"utf-8\"\n    )\n    # zip folder\n    \n    zip_base = meta_dir / \"payload\"\n    if (meta_dir / \"payload.zip\").exists(): (meta_dir / \"payload.zip\").unlink()\n    shutil.make_archive(str(zip_base), 'zip', folder)\n    try:\n        subprocess.check_call([\n            \"kaggle\",\"datasets\",\"create\",\"-p\",str(meta_dir),\"-r\",\"zip\",\"-o\",\n            \"--public\" if is_public else \"--private\"\n        ])\n    except subprocess.CalledProcessError:\n        subprocess.check_call([\n            \"kaggle\",\"datasets\",\"version\",\"-p\",str(meta_dir),\"-r\",\"zip\",\"-m\",\"auto snapshot\"\n        ])\n    return True\n\ndef kaggle_dataset_pull(dataset_slug, out_dir=\"downloaded\"):\n    _ensure_kaggle_config()\n    out = Path(out_dir); out.mkdir(parents=True, exist_ok=True)\n    subprocess.check_call([\"kaggle\",\"datasets\",\"download\",\"-d\",dataset_slug,\"-p\",str(out),\"-q\"])\n    for f in out.glob(\"*.zip\"):\n        shutil.unpack_archive(str(f), out)\n    return str(out.resolve())\n''').strip()+\"\\n\")\n\n# utils/sync_hf.py\nopen(BASE/\"utils/sync_hf.py\",\"w\").write(textwrap.dedent(\"\"\"\nimport os\nfrom huggingface_hub import HfApi, snapshot_download, create_repo\n\ndef hf_login_and_check(token=None):\n    token = token or os.getenv(\"HF_TOKEN\")\n    if not token:\n        raise RuntimeError(\"Provide HF token (arg or HF_TOKEN env).\")\n    api = HfApi(token=token)\n    me = api.whoami(token=token)\n    return api, me\n\ndef hf_snapshot_upload(repo_id, local_dir=\"checkpoints\", token=None, private=True):\n    api, _ = hf_login_and_check(token)\n    try:\n        create_repo(repo_id, private=private, exist_ok=True, token=api.token)\n    except Exception:\n        pass\n    from huggingface_hub import snapshot_upload\n    snapshot_upload(repo_id=repo_id, local_dir=local_dir, token=api.token, commit_message=\"auto snapshot\")\n    return True\n\ndef hf_snapshot_download(repo_id, local_dir=\"downloaded\", token=None):\n    api, _ = hf_login_and_check(token)\n    path = snapshot_download(repo_id=repo_id, local_dir=local_dir, token=api.token)\n    return path\n\"\"\").strip()+\"\\n\")\n\n# Notebook (skrócony; wszystkie kluczowe komórki)\nnb = {\n \"cells\": [\n  {\"cell_type\":\"markdown\",\"metadata\":{},\"source\":[\n    \"# Kaggle → Colab → Anywhere: DCGAN Starter\\n\",\n    \"Szybki przepływ z checkpointami i sync (Kaggle Datasets / HF Hub / W&B).\"\n  ]},\n  {\"cell_type\":\"code\",\"metadata\":{\"id\":\"setup\"},\"source\":[\n    \"import os, sys, subprocess\\n\",\n    \"def pip_install(pkgs): subprocess.check_call([sys.executable,'-m','pip','install','--quiet']+pkgs)\\n\",\n    \"try:\\n\",\n    \"    import torch, torchvision  # noqa\\n\",\n    \"except Exception:\\n\",\n    \"    pip_install(['torch','torchvision'])\\n\",\n    \"pip_install(['tqdm','omegaconf','wandb','kaggle','huggingface_hub'])\\n\",\n    \"print('Setup OK')\\n\"\n  ],\"execution_count\":None,\"outputs\":[]},\n  {\"cell_type\":\"code\",\"metadata\":{\"id\":\"folders\"},\"source\":[\n    \"import pathlib, platform, torch\\n\",\n    \"BASE=pathlib.Path('.')\\n\",\n    \"CKPT=(BASE/'checkpoints'); LOGS=(BASE/'logs'); DATA=(BASE/'data')\\n\",\n    \"for d in [CKPT,LOGS,DATA]: d.mkdir(parents=True, exist_ok=True)\\n\",\n    \"print('Python', platform.python_version(), 'Torch', torch.__version__, 'CUDA', torch.cuda.is_available())\\n\"\n  ],\"execution_count\":None,\"outputs\":[]},\n  {\"cell_type\":\"code\",\"metadata\":{\"id\":\"helpers\"},\"source\":[\n    \"import sys; sys.path.append(str((BASE/'..'/'..'/'gan-cross-platform-starter').resolve()))\\n\",\n    \"sys.path.append('gan-cross-platform-starter')\\n\",\n    \"from utils.checkpoint import latest_checkpoint, save_checkpoint, load_checkpoint\\n\",\n    \"from utils.sync_kaggle import kaggle_dataset_push, kaggle_dataset_pull\\n\",\n    \"from utils.sync_hf import hf_snapshot_upload, hf_snapshot_download\\n\",\n    \"print('Helpers OK')\\n\"\n  ],\"execution_count\":None,\"outputs\":[]},\n  {\"cell_type\":\"code\",\"metadata\":{\"id\":\"data\"},\"source\":[\n    \"from torchvision import datasets, transforms\\n\",\n    \"from torch.utils.data import DataLoader\\n\",\n    \"transform=transforms.Compose([transforms.Resize(32), transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\\n\",\n    \"train_set=datasets.MNIST(root='data', train=True, download=True, transform=transform)\\n\",\n    \"loader=DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)\\n\",\n    \"len(train_set), len(loader)\\n\"\n  ],\"execution_count\":None,\"outputs\":[]},\n  {\"cell_type\":\"code\",\"metadata\":{\"id\":\"model\"},\"source\":[\n    \"import torch, torch.nn as nn\\n\",\n    \"nz=64\\n\",\n    \"class G(nn.Module):\\n\",\n    \"    def __init__(self):\\n\",\n    \"        super().__init__(); self.net=nn.Sequential(\\n\",\n    \"            nn.ConvTranspose2d(nz,256,4,1,0,bias=False), nn.BatchNorm2d(256), nn.ReLU(True),\\n\",\n    \"            nn.ConvTranspose2d(256,128,4,2,1,bias=False), nn.BatchNorm2d(128), nn.ReLU(True),\\n\",\n    \"            nn.ConvTranspose2d(128,64,4,2,1,bias=False), nn.BatchNorm2d(64), nn.ReLU(True),\\n\",\n    \"            nn.ConvTranspose2d(64,1,4,2,1,bias=False), nn.Tanh())\\n\",\n    \"    def forward(self,z): return self.net(z)\\n\",\n    \"class D(nn.Module):\\n\",\n    \"    def __init__(self):\\n\",\n    \"        super().__init__(); self.net=nn.Sequential(\\n\",\n    \"            nn.Conv2d(1,64,4,2,1,bias=False), nn.LeakyReLU(0.2, inplace=True),\\n\",\n    \"            nn.Conv2d(64,128,4,2,1,bias=False), nn.BatchNorm2d(128), nn.LeakyReLU(0.2, inplace=True),\\n\",\n    \"            nn.Conv2d(128,256,4,2,1,bias=False), nn.BatchNorm2d(256), nn.LeakyReLU(0.2, inplace=True),\\n\",\n    \"            nn.Conv2d(256,1,4,1,0,bias=False))\\n\",\n    \"    def forward(self,x): return self.net(x).view(-1)\\n\",\n    \"device='cuda' if torch.cuda.is_available() else 'cpu'\\n\",\n    \"Gnet, Dnet = G().to(device), D().to(device)\\n\",\n    \"optG=torch.optim.Adam(Gnet.parameters(), lr=2e-4, betas=(0.5,0.999))\\n\",\n    \"optD=torch.optim.Adam(Dnet.parameters(), lr=2e-4, betas=(0.5,0.999))\\n\",\n    \"criterion=nn.BCEWithLogitsLoss()\\n\",\n    \"print('Model ready on', device)\\n\"\n  ],\"execution_count\":None,\"outputs\":[]},\n  {\"cell_type\":\"code\",\"metadata\":{\"id\":\"resume\"},\"source\":[\n    \"ckpt=latest_checkpoint('checkpoints')\\n\",\n    \"global_step=0\\n\",\n    \"if ckpt:\\n\",\n    \"    s=load_checkpoint(ckpt,'cpu')\\n\",\n    \"    Gnet.load_state_dict(s['G']); Dnet.load_state_dict(s['D'])\\n\",\n    \"    optG.load_state_dict(s['optG']); optD.load_state_dict(s['optD'])\\n\",\n    \"    global_step=int(s.get('step',0)); print('Resumed from', ckpt, 'step', global_step)\\n\",\n    \"else:\\n\",\n    \"    print('No checkpoint – start fresh')\\n\"\n  ],\"execution_count\":None,\"outputs\":[]},\n  {\"cell_type\":\"code\",\"metadata\":{\"id\":\"train\"},\"source\":[\n    \"from tqdm import tqdm; import wandb, os\\n\",\n    \"use_wandb=bool(os.getenv('WANDB_API_KEY'))\\n\",\n    \"if use_wandb: wandb.init(project=os.getenv('WANDB_PROJECT','gan-starter'), reinit=True)\\n\",\n    \"epochs=1; save_every=500; nz=64; device=device\\n\",\n    \"for epoch in range(epochs):\\n\",\n    \"    pbar=tqdm(loader, desc=f'Epoch {epoch+1}/{epochs}', ncols=100)\\n\",\n    \"    for real,_ in pbar:\\n\",\n    \"        real=real.to(device); bs=real.size(0)\\n\",\n    \"        z=torch.randn(bs,nz,1,1,device=device); fake=Gnet(z).detach()\\n\",\n    \"        lossD=criterion(Dnet(real), torch.ones(bs, device=device)) + \\\\\\n\",\n    \"              criterion(Dnet(fake), torch.zeros(bs, device=device))\\n\",\n    \"        optD.zero_grad(); lossD.backward(); optD.step()\\n\",\n    \"        z=torch.randn(bs,nz,1,1,device=device); fake=Gnet(z)\\n\",\n    \"        lossG=criterion(Dnet(fake), torch.ones(bs, device=device))\\n\",\n    \"        optG.zero_grad(); lossG.backward(); optG.step()\\n\",\n    \"        global_step+=1\\n\",\n    \"        if use_wandb: wandb.log({'lossD':lossD.item(), 'lossG':lossG.item(), 'step':global_step})\\n\",\n    \"        if global_step % save_every == 0:\\n\",\n    \"            import pathlib\\n\",\n    \"            p=pathlib.Path('checkpoints')/f'ckpt_{global_step:07d}.pt'\\n\",\n    \"            save_checkpoint(str(p), {'step':global_step,'G':Gnet.state_dict(),'D':Dnet.state_dict(),\\n\",\n    \"                                    'optG':optG.state_dict(),'optD':optD.state_dict(),'cfg':{'nz':nz}})\\n\",\n    \"print('Done. Last step =', global_step)\\n\"\n  ],\"execution_count\":None,\"outputs\":[]},\n  {\"cell_type\":\"markdown\",\"metadata\":{},\"source\":[\"### Push/Pull – Kaggle Datasets\"]},\n  {\"cell_type\":\"code\",\"metadata\":{\"id\":\"kaggle-sync\"},\"source\":[\n    \"import os\\n\",\n    \"slug=os.getenv('KAGGLE_DATASET_SLUG','your_kaggle_username/gan-checkpoints')\\n\",\n    \"try:\\n\",\n    \"    kaggle_dataset_push(slug, folder='checkpoints', title='GAN Checkpoints', is_public=False)\\n\",\n    \"    print('Pushed to Kaggle:', slug)\\n\",\n    \"except Exception as e:\\n\",\n    \"    print('Kaggle push skipped/error:', e)\\n\",\n    \"try:\\n\",\n    \"    out=kaggle_dataset_pull(slug, out_dir='downloaded')\\n\",\n    \"    print('Pulled to:', out)\\n\",\n    \"except Exception as e:\\n\",\n    \"    print('Kaggle pull skipped/error:', e)\\n\"\n  ],\"execution_count\":None,\"outputs\":[]},\n  {\"cell_type\":\"markdown\",\"metadata\":{},\"source\":[\"### Push/Pull – Hugging Face Hub\"]},\n  {\"cell_type\":\"code\",\"metadata\":{\"id\":\"hf-sync\"},\"source\":[\n    \"HF_TOKEN=os.getenv('HF_TOKEN'); HF_REPO=os.getenv('HF_REPO','username/gan-checkpoints')\\n\",\n    \"if HF_TOKEN:\\n\",\n    \"    try:\\n\",\n    \"        hf_snapshot_upload(HF_REPO, local_dir='checkpoints', token=HF_TOKEN, private=True)\\n\",\n    \"        print('Pushed to HF:', HF_REPO)\\n\",\n    \"    except Exception as e:\\n\",\n    \"        print('HF upload error:', e)\\n\",\n    \"    try:\\n\",\n    \"        path=hf_snapshot_download(HF_REPO, local_dir='downloaded_hf', token=HF_TOKEN)\\n\",\n    \"        print('Pulled from HF to:', path)\\n\",\n    \"    except Exception as e:\\n\",\n    \"        print('HF download error:', e)\\n\",\n    \"else:\\n\",\n    \"    print('Set HF_TOKEN to use HF sync')\\n\"\n  ],\"execution_count\":None,\"outputs\":[]},\n ],\n \"metadata\": {\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"}},\n \"nbformat\":4, \"nbformat_minor\":5\n}\n\nopen(BASE/\"notebooks/01_kaggle_colab_runpod.ipynb\",\"w\",encoding=\"utf-8\").write(json.dumps(nb, ensure_ascii=False, indent=2))\n\nprint(f\"✅ Utworzono projekt w: {BASE.resolve()}\")\nprint(\"Otwórz teraz notebooks/01_kaggle_colab_runpod.ipynb i uruchamiaj kolejne komórki.\")\n","metadata":{"_uuid":"d781647f-e7cd-434d-8b6f-fc4f21019612","_cell_guid":"6fc5613b-ca4b-4d2b-85c0-5ac44972413b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-08-09T15:18:11.103456Z","iopub.execute_input":"2025-08-09T15:18:11.103777Z","iopub.status.idle":"2025-08-09T15:18:11.130145Z","shell.execute_reply.started":"2025-08-09T15:18:11.103750Z","shell.execute_reply":"2025-08-09T15:18:11.129267Z"}},"outputs":[{"name":"stdout","text":"✅ Utworzono projekt w: /kaggle/working/gan-cross-platform-starter\nOtwórz teraz notebooks/01_kaggle_colab_runpod.ipynb i uruchamiaj kolejne komórki.\n","output_type":"stream"}],"execution_count":1}]}